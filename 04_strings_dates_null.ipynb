{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getenv(\"PYTHONPATH\", \"/app\")) #REVIEW\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/06 20:14:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "utils = Utils()\n",
    "spark = utils.get_spark_session()\n",
    "emp = utils.get_employee_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark functions can be accessed at: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The case when expression can be done in spark via the when() and otherwise() functions\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "emp.withColumn(\"gender_classified\", when(col(\"gender\") == \"Male\", \"M\").when(col(\"gender\") == \"Female\", \"F\").otherwise(None)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same using the expr() function:\n",
    "from pyspark.sql.functions import expr\n",
    "emp_gender = emp.withColumn(\"gender_classified\", expr(\"case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end\"))\n",
    "emp_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The replace function can be performed using the regexp_replace() from spark\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "emp.withColumn(\"altered_name\", regexp_replace(col(\"name\"), \"J\", \"Z\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert string dates to 'date' type, the function to date can be called, receiving the column object and the date format (like yyyy-MM-dd)\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "emp_date = emp.withColumn(\"dateColumn\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "emp_date.show()\n",
    "emp_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the string date format, the function date_format() can be used:\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "emp.withColumn(\"date_format\", date_format(col(\"hire_date\"), \"dd/MM/yyyy\")).withColumn(\"year_date\", date_format(col(\"hire_date\"), \"yyyy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are functions to return the current date and current timestamp: current_date(), current_timestamp()\n",
    "# The argument 'truncate' used in the function show() prevents the dataset showed to colapse values in order to show all columns \n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "emp.withColumn(\"current_date\", current_date()).withColumn(\"current_timestamp\", current_timestamp()).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle null values, the alias 'na' can be called on dataframes and then other functions can be used on top.\n",
    "# The drop function is an example of removing records in which any column is a null value:\n",
    "emp_gender.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coalesce operation can be executed through the coalesce() function. It can be a better alternative to dealing with null values:\n",
    "from pyspark.sql.functions import coalesce, col, lit\n",
    "\n",
    "emp_gender.withColumn(\"gender_classified\", coalesce(col(\"gender_classified\"), lit(\"-\"))).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
