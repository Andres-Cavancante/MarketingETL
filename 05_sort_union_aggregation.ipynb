{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getenv(\"PYTHONPATH\", \"/app\")) #REVIEW\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/07 18:59:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "utils = Utils()\n",
    "spark = utils.get_spark_session()\n",
    "emp = utils.get_employee_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Two dataframes are needed for the union so the limit() and subtract() functions can be used to filter half of the records from the employee data and then create a second dataframe\n",
    "# subtracting the records from the first one:\n",
    "# from pyspark.sql.functions import sub\n",
    "emp_data_1 = emp.limit(10)\n",
    "emp_data_2 = emp.subtract(emp_data_1)\n",
    "\n",
    "emp_data_1.show()\n",
    "emp_data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The union operation can be realized by the union() function. Just like in SQL, the column type and sequence must be the same in both datasets.\n",
    "union_emp = emp_data_1.union(emp_data_2)\n",
    "union_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union() will insert all records even if they are repeated between the datasets. The distinct() function can then be used to filter out distinct records:\n",
    "emp_data_1_union = emp_data_1.union(emp_data_1)\n",
    "# emp_data_1_union.show()\n",
    "emp_data_1_union.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The union by name behaviour is executed through the spark unionByName function:\n",
    "emp_data_2_unordered = emp_data_2.select(\"department_id\", \"gender\", \"age\", \"name\", \"salary\", \"hire_date\", \"employee_id\")\n",
    "\n",
    "# emp_data_1.union(emp_data_2_unordered).show() # Wrong union\n",
    "\n",
    "emp_data_1.unionByName(emp_data_2_unordered).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform an order by operation, the orderBy() function can be called with the method .asc or .desc on the column:\n",
    "from pyspark.sql.functions import col\n",
    "emp_data_1.orderBy(col(\"salary\").desc()).show()\n",
    "emp_data_2.orderBy(col(\"hire_date\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For aggregation, there are different functions available. First the groupBy function must be called in some column(s) and then the .agg()\n",
    "# Using the count() function to get the total employees per department id:\n",
    "# The alias() function can be used to rename the aggregated column. Otherwise, the column will be called function(column_name)\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "union_emp.groupBy(\"department_id\").agg(count(\"employee_id\").alias(\"number of employees\")).orderBy(col(\"number of employees\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is also a count() function over the dataframe, which will count the total number of records\n",
    "union_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_salary_double = union_emp.withColumn(\"salary\", col(\"salary\").cast(\"double\"))\n",
    "# emp_salary_double.printSchema()\n",
    "from pyspark.sql.functions import sum # Not importing this function will cause the code to interpret it as the python built in sum() function\n",
    "union_emp.groupBy(\"department_id\").agg(sum(\"salary\").alias(\"department_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter data based on an aggregation, the function where() can still be used:\n",
    "# Filter \n",
    "union_emp.groupBy(\"department_id\").agg(sum(\"salary\").alias(\"department_salary\")).where(\"department_salary > 50000\").orderBy(col(\"department_salary\").desc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
