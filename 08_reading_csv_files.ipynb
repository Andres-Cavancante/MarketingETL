{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getenv(\"PYTHONPATH\", \"/app\")) #REVIEW\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 10:05:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "utils = Utils()\n",
    "spark = utils.get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read data from a file, the read.format().load() function can be used. It is a built-in method in PySparkâ€™s DataFrameReader class that provides a\n",
    "# generic interface to load data from various sources:\n",
    "\n",
    "emp_df = spark.read.format(\"csv\").load(\"/app/inputs/emp.csv\") # REVIEW - I want to feed a relative path regardless of where I am running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options can be passed to read column names from the header (header=True) and to run throgh some examples of the data and infer types for the columns (inferSchema=True)\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"/app/inputs/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to specify the schema when loading data from a file, which will prevent job executions to collect the metadata:\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "\n",
    "emp_df_schema = spark.read.format(\"csv\").option(\"header\", True).schema(_schema).load(\"/app/inputs/emp.csv\") # If the header option is not passed as true, spark will read the header\n",
    "# as a register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark CSV documentation: https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_new is the same dataset as emp with some corrupted data:\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "\n",
    "spark.read.format(\"csv\").option(\"header\", True).schema(_schema).load(\"/app/inputs/emp_new.csv\").show()\n",
    "# There is a non double values in the 'salary' column and a non date value in the hire_date column\n",
    "# This is the default mode (PERMISSIVE), which allows for malformed fields setting them to null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark provides a metadata column of type string to access corrupted data in permissive mode, which is called '_corrupt_record':\n",
    "_schema_corr = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, _corrupt_record string\"\n",
    "\n",
    "spark.read.format(\"csv\").option(\"header\", True).schema(_schema_corr).load(\"/app/inputs/emp_new.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|         corr_record|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|          7|          101|James Johnson| 42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
      "|         11|          104|   David Park| 38|  Male|65000.0|      NULL|011,104,David Par...|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To alias the corrupt_record column, spark provides the columnNameOfCorruptRecord option\n",
    "# and to avoid calling option too many times, the options method can be called with a dictionary:\n",
    "\n",
    "options_dict = {\n",
    "    \"header\": True,\n",
    "    \"columnNameOfCorruptRecord\": \"corr_record\"\n",
    "}\n",
    "\n",
    "_schema_corr2 = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, corr_record string\"\n",
    "corrupt_emp = spark.read.format(\"csv\").options(**options_dict).schema(_schema_corr2).load(\"/app/inputs/emp_new.csv\")\n",
    "\n",
    "# The records where there are corrupted values can be filtered:\n",
    "corrupt_emp.where(\"corr_record is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the mode is set to DROPMALFORMED, spark ignores (drops) the whole corrupted records, which is the same of running permissive mode and filtering .where(\"corr_record is null\"):\n",
    "\n",
    "options_dict = {\n",
    "    \"header\": True,\n",
    "    \"mode\": \"DROPMALFORMED\"\n",
    "}\n",
    "\n",
    "spark.read.format(\"csv\").options(**options_dict).schema(_schema).load(\"/app/inputs/emp_new.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With mode set to FAILFAST, Spark throws an exception when it meets corrupted records:\n",
    "options_dict = {\n",
    "    \"header\": True,\n",
    "    \"mode\": \"FAILFAST\"\n",
    "}\n",
    "spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(_schema).load(\"/app/inputs/emp_new.csv\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
