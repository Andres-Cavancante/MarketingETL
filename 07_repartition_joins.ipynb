{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getenv(\"PYTHONPATH\", \"/app\")) #REVIEW\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/09 21:12:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "utils = Utils()\n",
    "spark = utils.get_spark_session()\n",
    "emp = utils.get_employee_data()\n",
    "dptm = utils.get_department_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dptm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to change the number of partitions in a DataFrame in spark by utilizing repartition() or coalesce()\n",
    "# repartition redistributes the data evenly across partitions. Useful when increasing the number of partitions or balancing data:\n",
    "# This is useful when preparing for parallel computation\n",
    "print(emp.rdd.getNumPartitions())\n",
    "less_partitions_emp = emp.repartition(4)\n",
    "more_partitions_emp = emp.repartition(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(less_partitions_emp.rdd.getNumPartitions())\n",
    "print(more_partitions_emp.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce only reduces the number of partitions. But it avoids full shuffling by merging partitions instead of redistributing:\n",
    "# This is useful when writting writing a final output\n",
    "repartitioned_emp = emp.coalesce(2) # pyspark has a coalesce() function to be called on columns, which is different from this coalesce to be called on dataframes\n",
    "print(repartitioned_emp.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The repartition can be done across specified columns. Repartitioning the data into 4 partitions by 'department_id' column.:\n",
    "emp.repartition(4, \"department_id\").rdd.getNumPartitions() # N columns can be specified to partition by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to visualize the data partitioning with spark_partition_id:\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.repartition(4, \"department_id\").withColumn(\"partition_id\", spark_partition_id()).show() # Here every department_id is in a single partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join operations can be done utiling the join function:\n",
    "emp_join = emp.join(dptm, how=\"inner\", on=emp.department_id==dptm.department_id)\n",
    "emp_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select the fields, aliasing can be used since there are ambiguous fields:\n",
    "emp_join.select(emp.name, dptm.department_name, dptm.department_id, emp.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the alias can be explicitly defined:\n",
    "emp.alias(\"e\").join(dptm.alias(\"d\"), how=\"inner\", on=emp.department_id==dptm.department_id).select(\"e.name\", \"d.department_name\", \"d.department_id\", \"e.salary\").show() # The quotes are\n",
    "# necessary because 'e.name' and the remaining are column names now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INNER JOIN: Matches only existing keys in both DataFrames:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"inner\").show()\n",
    "\n",
    "LEFT JOIN: All left DataFrame rows, matched ones from right:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"left\").show()\n",
    "\n",
    "RIGHT JOIN: All right DataFrame rows, matched ones from left:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"right\").show()\n",
    "\n",
    "FULL OUTER JOIN: All rows from both DataFrames:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"full\").show()\n",
    "\n",
    "LEFT SEMI JOIN: Returns only left rows where a match exists:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"left_semi\").show()\n",
    "\n",
    "LEFT ANTI JOIN: Returns only left rows where no match exists:\n",
    "\n",
    "df.join(df2, on=\"id\", how=\"left_anti\").show()\n",
    "\n",
    "CROSS JOIN: Cartesian product of both tables:\n",
    "\n",
    "df.crossJoin(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cascade conditions in a join operation, the & logical operation can be used:\n",
    "emp.join(\n",
    "    dptm,\n",
    "    how=\"left\",\n",
    "    on=(emp.department_id==dptm.department_id) & ((emp.department_id == \"101\") | (emp.department_id == \"102\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter out the records with null department_ids, the isNotNull() can be used:\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp.join(\n",
    "    dptm,\n",
    "    how=\"left\",\n",
    "    on=(emp.department_id==dptm.department_id) & ((emp.department_id == \"101\") | (emp.department_id == \"102\"))\n",
    ").filter(dptm.department_id.isNotNull()).show() #Alternatively, the isNull() function can be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
